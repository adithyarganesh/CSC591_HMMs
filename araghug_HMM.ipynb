{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5: HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from hmmlearn import hmm\n",
    "\n",
    "np.random.seed(200314659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required libraries, we generate the state transition matrix and emission probability matrix by leveraging the random library. I also found a library in python that creates a model with the probability matrices, so that has been initialized as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating P_ij\n",
    "\n",
    "P = np.random.random((4, 4))\n",
    "for i in range(4):\n",
    "    P[i] = P[i]/sum(P[i])   \n",
    "B = np.random.random((4, 3))\n",
    "for i in range(4):\n",
    "    B[i] = B[i]/sum(B[i])\n",
    "pi = [1, 0, 0, 0]   \n",
    "model = hmm.MultinomialHMM(n_components=4, algorithm='viterbi', random_state=200314659, n_iter=10, tol=0.01)\n",
    "model.startprob_ = np.array(pi)\n",
    "model.transmat_ = np.array(P)\n",
    "model.emissionprob_ = np.array(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "The data set generation has been performed as explained in the document. The same has been verified for correctness by running the matrices in the hmmlearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 263, 4: 258, 3: 241, 2: 238})\n",
      "Counter({3: 473, 1: 329, 2: 198})\n"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "q = 1\n",
    "states = [1]\n",
    "\n",
    "r = np.random.choice([1,2,3,4], p=P[q-1])\n",
    "O = [np.random.choice([1,2,3], p=B[r-1])]\n",
    "\n",
    "for i in range(999):\n",
    "    q = np.random.choice([1,2,3,4])\n",
    "    r = np.random.choice([1,2,3,4], p=P[q-1])\n",
    "    O.append(np.random.choice([1,2,3], p=B[r-1]))\n",
    "    states.append(q)\n",
    "\n",
    "print(Counter(states))\n",
    "print(Counter(O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above implementation uses no major libraries. However, there were libraries to perform the same - this has been implemented in the block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 370, 1: 325, 3: 219, 0: 86})\n",
      "Counter({2: 458, 0: 323, 1: 219})\n"
     ]
    }
   ],
   "source": [
    "model_obs, model_states = model.sample(1000)\n",
    "print(Counter(model_states.flatten()))\n",
    "print(Counter(model_obs.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Given  that  you  know  the  parameters  of  the  HMM,  calculate  the  probability ùëù(ùëÇ|ùúÜ)  that  the sequence of observations \n",
    "\n",
    "ùëÇ = 123312331233 came from the HMM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016158508588046218\n"
     ]
    }
   ],
   "source": [
    "def mul(x, y): \n",
    "    return x * y\n",
    "\n",
    "P_mle = {}\n",
    "B_mle = {}\n",
    "for i, j in enumerate(map(list, zip(*P))):\n",
    "    P_mle[i+1] = j\n",
    "for i, j in enumerate(map(list, zip(*B))):\n",
    "    B_mle[i+1] = j\n",
    "\n",
    "pi = pd.DataFrame(data={1: [1], 2: [0], 3: [0], 4: [0]})\n",
    "\n",
    "observations = [1,2,3,3]\n",
    "pi_states = [1,2,3,4]\n",
    "\n",
    "score = 0\n",
    "all_hmms = list(product(*(pi_states,) * len(observations)))\n",
    "\n",
    "for chain in all_hmms:\n",
    "    e_chain = list(zip(chain, [1] + list(chain)))\n",
    "    p_hs = list(map(lambda x: pd.DataFrame(data=P_mle, index=pd.Index([1,2,3,4], name='Rows')).loc[x[1], x[0]], e_chain))\n",
    "    p_hs[0] = pi[chain[0]]\n",
    "        \n",
    "    e_obser = list(zip(observations, chain))\n",
    "    p_obs = list(map(lambda x: pd.DataFrame(data=B_mle, index=pd.Index([1,2,3,4], name='Rows')).loc[x[1], x[0]], e_obser))\n",
    "    \n",
    "    score += reduce(mul, p_obs) * reduce(mul, p_hs)\n",
    "\n",
    "print(score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p([1, 2, 3, 3]) = 0.016158508588046214 \n"
     ]
    }
   ],
   "source": [
    "observations = [1,2,3,3]\n",
    "oo = [i-1 for i in observations]\n",
    "obs_sequence = np.array([oo]).T\n",
    "p = np.exp(model.score(obs_sequence))\n",
    "print(\"p({}) = {} \".format([observations[i] for i in obs_sequence.T[0]], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given sequence, when I tried running it on the code that followed the logic for Lambda calculation it took a lot of time. Thus, I used a smaller sample to check for correctness and ran the same observations on the model defined previously. The following output was obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1253085204918785"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(obs_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2425228886660963e-06\n"
     ]
    }
   ],
   "source": [
    "observations = [1,2,3,3,1,2,3,3,1,2,3,3]\n",
    "oo = [i-1 for i in observations]\n",
    "obs_sequence = np.array([oo]).T\n",
    "score = np.exp(model.score(obs_sequence))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate this, we marginalize all possible chains of the hidden variables and determmine the MLE value or the score that the model determines by computing the product of all probabilities related to the observables and the the product of all probabilities of transitioning from x at t to x at t + 1.\n",
    "\n",
    "From some further analysis, I also noticed that the score values for all possible combinations of an input observation adds up to 1 verifying the value obtained above to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Estimation of most probable sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [1,2,3,3,1,2,3,3,1,2,3]\n",
    "oo = [i-1 for i in observations]\n",
    "\n",
    "_, sequences = model.decode(np.array([oo]).T)\n",
    "sequence = []\n",
    "\n",
    "for s in sequences:\n",
    "    sequence.append(states[int(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the observations:  [1, 2, 3, 3, 1, 2, 3, 3, 1, 2, 3] we get sequence:  [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print( \"For the observations: \", observations, \"we get sequence: \", sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hmmlearn model allows us to decode the observations and identify the hiddenstates to determine the corresponding values. The observations and their corresponding sequences have been identified and displayed.\n",
    "\n",
    "It was understood that the Viterbi algorithm was commonly used  for the estimation of hidden state values using dynamic programming. The model.decode() function performs the same and it uses the alpha (highest joint probability) values for calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Training the hmm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = hmm.MultinomialHMM(n_components=4, algorithm='viterbi', random_state=200314659, n_iter=10, tol=0.01)\n",
    "new_model = new_model.fit(model_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_P =  [[0.35736501 0.1057952  0.2281218  0.30871799]\n",
      " [0.05958057 0.35752997 0.01098403 0.57190544]\n",
      " [0.05418169 0.19766124 0.7012921  0.04686497]\n",
      " [0.04710583 0.54091908 0.32629607 0.08567901]]\n",
      "new_P =  [[0.32627748 0.30914319 0.17984501 0.18473432]\n",
      " [0.3795852  0.33625037 0.1399961  0.14416833]\n",
      " [0.3133156  0.29876962 0.19315349 0.19476129]\n",
      " [0.37708467 0.33225828 0.14437024 0.14628681]]\n",
      "old_B =  [[0.4857143  0.10391807 0.41036764]\n",
      " [0.53486341 0.15204137 0.31309522]\n",
      " [0.20448086 0.37904062 0.41647852]\n",
      " [0.15550001 0.09513801 0.74936198]]\n",
      "new_B =  [[0.31341797 0.05070164 0.63588039]\n",
      " [0.50957404 0.01426692 0.47615904]\n",
      " [0.02230523 0.6630869  0.31460787]\n",
      " [0.27853961 0.53070303 0.19075736]]\n",
      "old_pi =  [1 0 0 0]\n",
      "new_pi =  [1.06782351e-02 9.87957065e-01 1.66165708e-14 1.36469980e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"old_P = \",P)\n",
    "print(\"new_P = \", new_model.transmat_)\n",
    "print(\"old_B = \", B)\n",
    "print(\"new_B = \", new_model.emissionprob_)\n",
    "print(\"old_pi = \", model.startprob_)\n",
    "print(\"new_pi = \", new_model.startprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously generated sequence is used to create the emission and transmission probability values along with pi. We notice that there is a high unbalance in the values generated for P and B but the pi value is completely differnt in the new model. We also notice that it implements the Baum-Welch algorithm to do the same after calling the fit() function with the previously generated observations.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "We can concude that the hidden states converges to a stationary state distribution and doesnt store the current observation.\n",
    "\n",
    "Ref:\n",
    "\n",
    "http://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/    \n",
    "https://brilliant.org/wiki/stationary-distributions/    \n",
    "https://towardsdatascience.com/hidden-markov-model-implemented-from-scratch-72865bda430e    \n",
    "https://hmmlearn.readthedocs.io/en/latest/tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
